{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from random import choice\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.exceptions import ProxyError, SSLError, ConnectTimeout\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_df = pd.read_csv('./data/nasdaq_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_list(only_https = False):\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    # dom = etree.HTML(str(soup))\n",
    "\n",
    "    html_table = soup.find('table', attrs={'class': 'table table-striped table-bordered'})\n",
    "    df_table = pd.read_html(str(html_table))[0]\n",
    "\n",
    "    proxy_server_list = []\n",
    "    for i in range(len(df_table)):\n",
    "        ip = df_table.loc[i, 'IP Address']\n",
    "        port = df_table.loc[i, 'Port']\n",
    "        https = df_table.loc[i, 'Https']\n",
    "\n",
    "        if (only_https and https == \"yes\") or not only_https:\n",
    "            server = f\"{ip}:{port}\"\n",
    "            proxy_server_list.append(server)\n",
    "\n",
    "    return proxy_server_list\n",
    "\n",
    "\n",
    "PROXY_SERVER_LIST = get_proxy_list(only_https=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 5\n",
    "\n",
    "def scrape_marketwatch(symbol: str, use_proxy = False):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    URL = f\"https://www.marketwatch.com/investing/stock/{symbol.lower()}/financials/income\"\n",
    "\n",
    "    if use_proxy:\n",
    "        proxy_server_list = PROXY_SERVER_LIST[:]\n",
    "        \n",
    "        while proxy_server_list:\n",
    "            proxy_server = choice(proxy_server_list)\n",
    "            proxies = {\"http\": proxy_server, \"https\": proxy_server}\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(\n",
    "                    URL,\n",
    "                    headers=headers,\n",
    "                    proxies=proxies,\n",
    "                    timeout=10,\n",
    "                )\n",
    "            except (ProxyError, SSLError, ConnectTimeout) as e:       \n",
    "                proxy_server_list.remove(proxy_server)\n",
    "                print(f'{symbol}: Retry on another proxy, {len(proxy_server_list)}')\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                URL,\n",
    "                headers=headers,\n",
    "                timeout=10,\n",
    "            )\n",
    "        except requests.exceptions.RequestException as e:       \n",
    "            print(f'{symbol}: Connection failed. {e}')\n",
    "            return None\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text)\n",
    "\n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            financial_table = soup.find(\n",
    "                \"table\", attrs={\"class\": \"table table--overflow align--right\"}\n",
    "            )\n",
    "            df = pd.read_html(str(financial_table))[0]\n",
    "            df = df.drop(df.columns[[-1]], axis=1)  # drop 5-year trend column\n",
    "        except:\n",
    "            print(f\"{symbol}: Parsing failed. Retry {attempt + 1}\")\n",
    "            continue\n",
    "        else:\n",
    "            return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_scrape(start: int, end: int, use_proxy = False):\n",
    "    result = pd.DataFrame()\n",
    "    for i in tqdm(range(start, end)):\n",
    "        name = nasdaq_df.loc[i, 'Name']\n",
    "        symbol = nasdaq_df.loc[i, 'Symbol']\n",
    "        industry = nasdaq_df.loc[i, 'Industry']\n",
    "        \n",
    "        df = scrape_marketwatch(symbol, use_proxy)\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        # add multiindex level\n",
    "        df = pd.concat([df], keys=[industry], names=['Industry'])\n",
    "        df = pd.concat([df], keys=[symbol], names=['Symbol'])\n",
    "        df = pd.concat([df], keys=[name], names=['Name'])\n",
    "        \n",
    "        result = pd.concat([result, df])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = [(i, min(i + 500, len(nasdaq_df))) for i in range(0, len(nasdaq_df), 500)]\n",
    "print(work_list)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = [executor.submit(thread_scrape, work[0], work[1]) for work in work_list]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        # print(result)\n",
    "        df = pd.concat([df, result])\n",
    "\n",
    "df.to_csv('./data/nasdaq_full2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _conv_to_float(s):\n",
    "    if s == '-' or not isinstance(s, str):\n",
    "        return None\n",
    "\n",
    "    if s[0] == '(' or s[-1] == ')':\n",
    "        s = s.replace('(', '')\n",
    "        s = s.replace(')', '')\n",
    "    if s[-1] == '%':\n",
    "        s = s.replace('%', '')\n",
    "    if s[-1] in list('BMK'):\n",
    "        powers = {'B': 10 ** 9, 'M': 10 ** 6, 'K': 10 ** 3, '': 1}\n",
    "        m = re.search(\"([0-9\\.]+)(M|B|K|)\", s)\n",
    "        if m:\n",
    "            val, mag = m.group(1), m.group(2)\n",
    "            return float(val) * powers[mag]\n",
    "    try:\n",
    "        result = float(s)\n",
    "    except:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nasdaq_full_raw2.csv')\n",
    "\n",
    "conv_list =[str(year) for year in range(2016, 2023)]\n",
    "for col in conv_list:\n",
    "    df[col] = df[col].apply(_conv_to_float)\n",
    "df.to_csv('./data/nasdaq_full_proc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jupy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f2c90995bf61f046707788a0b73ac4bd8c7fa3f699490437d16f94fef7b744e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
